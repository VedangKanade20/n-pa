import nltk
import spacy
import re
from nltk.tokenize import word_tokenize

def tokenize_nltk(text):
    nltk.download('punkt')
    return word_tokenize(text)

def tokenize_spacy(text):
    nlp = spacy.load("en_core_web_sm")
    doc = nlp(text)
    return [token.text for token in doc]

def tokenize_re(text):
    return re.findall(r'\b\w+\b', text)

def read_text_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        return file.read()

def main():
    file_path = input("Enter the path of the text file: ")
    text = read_text_file(file_path)

    print("\nTokenization using NLTK:")
    print(tokenize_nltk(text))

    print("\nTokenization using spaCy:")
    print(tokenize_spacy(text))

    print("\nTokenization using re module:")
    print(tokenize_re(text))

if __name__ == "__main__":
    main()
